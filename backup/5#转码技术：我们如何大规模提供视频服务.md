# 转码技术：我们如何大规模提供视频服务

转码技术：我们如何大规模提供视频服务

原文地址 [www.egnyte.com](https://www.egnyte.com/blog/post/transcoding-how-we-serve-videos-at-scale/)

> 写在前面的话：最近有时间贡献一下著名的《系统设计学习》 [awesome-database-learning](https://github.com/pingcap/awesome-database-learning)翻译，大家就凑乎看吧，感兴趣可以直接读原文，译文我放在这里：

随着社交网络和移动设备的兴起，视频已成为最主要的内容类型之一。多项研究表明，吸引新一代消费者的视频能够获得最高的用户参与度。

另外，人们对于所消费的内容有一个固有的期望，即不仅要立即获得，而且在用户从一个设备跳转到另一个设备时要保持连续性。这使得视频存储、传输和播放成为一个具有挑战性的问题，需要以成本效益的方式进行大规模解决。

在互联网上流媒体和播放视频比下载其他文件类型并在本地应用程序中加载它们要复杂得多。提供和播放视频时可能遇到的各种问题包括：

1.  视频文件的大小通常在几百兆到几个千兆之间。这使得下载整个视频并播放它变得困难。用户也可能不想看整个视频，只想看其中的一些部分。

    等待整个文件下载完毕是没有意义的。快点！

2.  播放视频的本地支持非常有限。例如，Windows 系统无法播放 mov 文件，而苹果设备则无法播放 wmv 文件。

3.  同一种容器类型的视频（mp4、mov、wmv 等）可能使用不同的编解码器，而并非所有编解码器都可以在所有设备上播放。

4.  手机设备的带宽有限，高分辨率视频不适合快速流媒体播放。

上述问题的解决方案是视频转码，它是将视频文件从一种格式转换为另一种格式，并使其能够在不同的平台和设备上观看的过程。视频转码就像是给视频换了一身新衣服，让它能够适应不同的环境和设备。

在流媒体中有两种选项：自适应码率和静态码率。 自适应码率就像是一位聪明的流量大神，它会根据你的网络状况自动调整视频的清晰度，确保你能够流畅观看。就像是一位随机应变的小天使，总是能给你最好的观影体验。 而静态码率就像是一位固执己见的老顽固，它会始终保持视频的固定清晰度，不管你的网络好坏。有时候，它可能会让你在观看高清视频时感到有些卡顿，就像是一位不愿妥协的小顽童。 所以，如果你想要一个智能、顺畅的观影体验，那就选择自适应码率吧！毕竟，谁不想拥有一个能随时调整的流量大神呢？

我们感兴趣的自适应格式被称为 HTTP Live Streaming（HLS），它已成为自适应比特率视频的标准。

下一步是决定何时进行视频转码 - 我们考虑了两个选项：

1.  视频上传后，我们会异步转码。这意味着我们会对所有上传的视频进行转码，哪怕有些视频可能永远都不会被预览。为了在预览可用性和成本之间取得平衡，我们需要一个超级智能的架构。
2.  当用户点击“播放”时，实时转码视频。这将减少需要转码的视频量，但是对于大型视频的视频转码可能会很慢，因此需要高端设备甚至昂贵的 GPU 来满足用户的期望。

我们的数据团队帮助我们找到了答案。原来我们的数据中有 20%是视频，而且有 40%的用户使用移动设备，所以我们开始研究他们如何分享、预览和流媒体视频。 哇！原来我们的数据中有这么多视频啊！而且有这么多用户都是用手机看视频！所以我们就开始研究他们是怎么分享、预览和看视频的。

我们发现大多数用户在视频上传后至少几个小时后才预览视频，这给了我们足够的时间进行异步转码。

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134803.png)

接下来，我们寻找了实际流传播最频繁的视频类型：

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134793.png)

我们非常惊喜地发现，大多数流媒体视频都是 mp4 格式的，这种格式在大多数设备上都有原生支持，所以我们可以优先转码.mov 格式的视频，并在转码尚未完成时回退到 mp4 的原生支持。

所以通过一些数据分析，我们决定在上传或分享链接创建后异步运行视频转码器，并围绕此构建一个基于 SLA 的架构。

问题及其解决方案

拆解视频流传输问题，我们构建了以下流程：

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134885.png)

####

传输和存储大型视频

Egnyte 对象存储已经做得很好了，我们有用户存储的 PB 级内容，包括视频。 （打印结果）：Egnyte 对象存储已经做得很好了，我们有用户存储的“宠物字节”级内容，包括视频。

####

通知转码作业

一旦视频文件上传完成，我们就需要通知转码器，所以我们需要一个好的消息传递系统。我们已经在使用 RabbitMQ、Redis Pub/Sub 和 Scribe，但是这个使用场景有所不同，因为我们希望视频转码器可以部署在世界上的任何地方。

这可以是在 Egnyte 管理的数据中心中，也可以是在公共云中，以便在负载激增时进行扩展。这意味着我们需要一个全球消息系统，比如 Google Pub/Sub 或 Azure ServiceBus，以便在上传新文件时通知转码器。

用消息总线来传递信息也有一些问题。视频转码任务可能要运行好几个小时，我们需要在这期间一直占用着这个消息的“租约”。

在转码过程中，消息总线不应将相同的消息重新发送给另一个客户端，因此我们需要更长的租约时间。

举个例子，我们最开始使用了最新的 Google PubSub 客户端，但后来不得不回退到之前的版本，因为最新版本不允许我们延长租约的时长。

#### 视频转码

这是一个在大规模情况下难以解决的问题。我们最初尝试将视频转码外包给一个有现成产品的合作伙伴，但成本太高了。事实上，我们的分析显示，自己内部运行视频转码器要便宜 90%。

这让我们得出结论，为了合规和成本的原因，我们需要亲自动手来内部建设这个项目。

为了将视频转码为 HLS 格式，我们选择了 FFmpeg。FFmpeg 是一套用于处理多媒体内容（如音频、视频、字幕和相关元数据）的库和工具集合。FFmpeg 是目前最好的免费软件，非常适合视频转码，并且对 HLS 格式有很好的支持。

FFmpeg 可以原生地创建不同分辨率的各种 HLS 片段。

经过详细测试，我们选择了以下的 ffmpeg 命令行参数：

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134245.png)

我们的视频转码工作会输出以下文件：

1.  流.m3u8 - 包含 Egnyte 支持的所有分辨率和播放列表的主播放列表，以及它们的带宽推荐。
2.  .m3u8：对于 Egnyte 支持的每个分辨率，在 stream.m3u8 中列出，例如 640x480，会生成一个特定分辨率的 480p.m3u8 播放列表。
3.  视频片段：为每个支持的分辨率创建更小的固定时间视频片段。与\*.m3u8 文件不同，这些实际上是视频文件，而不是元数据文件。

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134800.png)

以下是生成的示例 m3u8 文件：

流媒体.m3u8：

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134202.png)

480p.m3u8 是一种视频流媒体格式，其中的"480p"表示视频的分辨率为 480 像素。m3u8 是一种播放列表文件格式，用于指定视频流的地址和相关信息。

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134638.png)

720p.m3u8：高清视频播放列表。

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134638.png)

**可扩展性，就是说能不能随着需求的增加而增加。比如，你有一个小型的网站，每天只有几十个访问量，那么你的服务器可能能够应付得了。但是，如果你的网站突然爆红，每天有上万甚至上百万的访问量，你的服务器可能就会崩溃。这就是缺乏可扩展性的表现。所以，可扩展性非常重要，它能够帮助你的系统在需求增加时保持稳定**

**‍**我们接下来需要解决的挑战是根据需求扩展视频转码任务的规模。视频转码任务的运行时间非常不对称，这意味着处理较小视频的任务可能很快完成，但较大的任务可能需要一个小时，从而使其他任务排队等待。

一开始我们就明白，我们需要在一个能根据当前积压和季节性动态调整的平台上部署我们的视频转码任务。就像是在排队买奶茶一样，我们需要一个智能的系统，能根据人潮和季节的变化，灵活调整转码任务的处理速度。这样才能保证我们的视频能够及时转码完成，不会出现拥堵的情况。

我们首选的是利用无服务器架构来部署视频转码器。我们喜欢这个想法，可以在云函数上构建，自动扩展，无需担心积压和负载峰值。

然而，我们很快意识到要大规模利用这些服务还为时尚早。视频转码是一项对 CPU 要求很高的操作，需要特定的硬件，如配备足够内存和安装了原生 ffmpeg 的专用 CPU（甚至是 GPU）。

这个视频是讲如何在无服务器环境下构建这个项目的，但我们发现使用 Kubernetes 更适合我们，所以我们创建了 Alpine docker 容器，里面包含了 FFmpeg + Python，并在 Kubernetes 上进行了部署。

而且，由于我们已经在 Egnyte 中使用了 Kubernetes，所以学习曲线较小。

我们发现使用 GPU 可以加快视频转码的速度，但这样做并不划算。在速度和成本之间取得最佳平衡的方法是为每个视频转码作业分配 4 个 CPU。使用 4 个 CPU，我们能够以视频播放时间的 25-40%的速度处理视频。

换句话说，一个 1 小时的视频需要大约 15-25 分钟来转码。增加更多的 CPU 到视频转码作业并不能产生线性的好处，所以最好每个作业使用 4 个 CPU，并提供更多的作业。

我们搞了个自动扩展器，它会根据视频转码队列的积压情况，自动增加或减少作业的数量。就像是有个小助手，时刻关注着队列的情况，随时调整工作量。

自动缩放器是管理成本和用户体验最重要的部分。没有自动缩放器，就无法在不过度配置和不断增加成本的情况下提供良好的转码服务水平协议（SLA）。

转码器还面临着一个挑战 - 它们偶尔会卡住。为了解决这个问题，我们在转码器的 Docker 容器中内置了健康监控。如果健康检查失败，Kubernetes 集群会重新启动容器。

判断转码器是否卡住并不容易 - 我们在这方面有一些小问题。

-     如果转码器在一段时间内没有从消息总线主题接收到任何消息，并且主题中仍有未处理的消息，我们将无法通过健康检查
-     如果FFmpeg作业卡住了，即不再生成任何HLS片段但仍在运行，我们的健康检查就会失败。

重启转码器会在租约到期后将其暂存的消息放回待处理队列，并由下一个可用的转码器接手处理。

为了适应目标公有云环境，我们还需要对服务进行部署优化。在我们构建这项服务的同时，Google 在其 Kubernetes 集群（GKE）中增加了对可抢占节点的支持。

抢占式节点比普通节点便宜，但缺点是它们可能经常被重新启动，以便在 Google 的不同基础设施 Pod 上重新定位。 抢占式节点：便宜但爱搬家。

考虑到我们构建的转码服务需要在全球范围内部署，并且根据我们的需求在本地和公共云上具备可扩展性，GKE 的可抢占节点非常适合我们处理特定工作负载，并进一步降低了成本。

我们在 GKE 实例中同时使用了抢占式节点和常规节点，以确保我们所需的转码器数量始终可用，无论抢占式节点是否可用，同时通过在可用时使用抢占式节点来降低成本。

####

存储转码后的视频

就像上面所看到的，转码是一项非常昂贵的操作，因此保留转码对象一段合理的时间是有意义的。根据上传的视频质量，转码后的视频大小可能是原始大小的 20%到 50%。

在我们的规模下，这也是一个有趣的问题。

我们需要一个长期存储选项来存放转码后的视频；一个耐用且能扩展到处理 PB 级数据的选项。对象的生命周期可能是不确定的，有些情况下可能只有几天，但大多数情况下可能是几个月。

为了解决这个问题，我们需要一个我们称之为“派生对象存储”的东西 - 一个能够存储派生对象（如转码视频）的对象存储，其生命周期取决于用户上传的主要对象，但可用性可以稍微放松一些。

在当时，我们的核心对象存储系统还不能存储衍生的对象，但我们已经在将其扩展到能够处理数十亿个对象和数 PB（PB 字节）的数据。我们深谙如何有效管理和监控这一系统，它成了存放转码对象的理想之选。于是，我们开始在核心对象存储系统中增加“派生对象存储”这一功能，进一步解决了转码工作中的一个重要问题。

#### 为您呈现转码后的视频

给用户提供流媒体视频与提供 API 或 Web UI 请求是不同的。视频片段通常较大且持续时间较长，可能会在视频走红时突然爆发。

鉴于视频请求的不可预测性，我们决定将视频服务与我们的常规流量完全隔离开来。

我们的视频服务基于 OpenResty，所有身份验证和视频发现都是用 Lua 编写的。我们在专用基础设施上部署了带有缓存的视频服务，由专用域名（例如 media.egnyte.com）进行前端访问。

这个视频服务与我们的主要服务没有共享任何基础设施组件，比如防火墙、交换机和 ISP 链接，这使得我们能够根据视频需求来扩展视频服务并对用户进行速率限制。

由于视频服务节点主要负责身份验证、授权和视频服务，我们必须根据当前运行的连接数量来扩展它们。为了使用 Kubernetes HPA 实现这一目标，我们需要将 OpenResty 中的自定义指标“active_connections”发送到 Stackdriver。

我们通过在视频服务 Pod 中部署两个容器以及视频服务 OpenResty 来实现这一点

-     nginx/nginx-prometheus-exporter - 将nginx指标导出到Prometheus（一种监控工具）
-     gcr.io/google-containers/prometheus-to-sd - 将Prometheus指标导出到Stackdriver

一旦我们在 Stackdriver 中有了活跃连接的指标，我们就部署了一个自定义指标的 HPA，以便在活跃连接增加时扩展视频服务的 Pod。

最后，当放在一起时，这就是我们整体架构的样子：

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134168.png)

#### 监控中

监控对于任何生产规模的系统都至关重要，视频转码和视频服务也不例外。

有几个要监控的事情，我们使用由我们的转码器提供的指标以及挖掘其他组件（如 pubsub 主题和视频服务 OpenResty 服务器）的统计数据来进行监控。

从转码的角度来看，衡量的关键指标是积压量，因为这直接影响到我们的用户。我们监控发布-订阅主题中未处理的消息数量。

随时，如果队列中有太多等待处理的消息，自动缩放器会增加更多的转码器。如下所示，随着未处理消息的数量增加，我们会增加转码器的数量。

随着积压的减少，我们减少了转码器的数量。

消息数量：

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134121.png)

转码器数量：

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134185.png)

我们的 SRE 团队在积压超过一定阈值且由于最大转码器数量限制，自动缩放器无法跟上时也会收到通知。然后，SRE 团队会做出明智的决策，是否需要提高自动缩放器的限制。

我们时刻关注转码速度和转码延迟，这样我们就能知道用户体验如何啦。不要让视频卡，不然用户会很不爽！

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134384.png)

我们还会监控已处理的视频，以了解流量模式和成本。

![](https://gengnuo-1257145452.cos.ap-beijing.myqcloud.com/picgo/202312220134509.png)

这就是我们如何以经济高效、合规的方式解决了将视频流式传输到用户设备的长期问题。

这个项目的下一个阶段是找到最佳方法，以实时利用 GPU 进行高性能视频转码，从而立即提供新上传视频的服务。


---

* Link: https://github.com/sumulige/system-design-resources/issues/5
* Labels: `Transcoding`
* Creation Date: 2023-12-23T07:48:10Z
